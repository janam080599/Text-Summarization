# -*- coding: utf-8 -*-
"""IT412_Group3_Project_Text-Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ki89HAYdrDYpuweczMxq_w6LOhIFj6DM

**LexRank**
"""

#LexRank Summarizer
class LexRankSummarizer():  
    threshold = 0.1
    epsilon = 0.1
    _stop_words = frozenset()
    @property
    def stop_words(self):
        return self._stop_words

    @stop_words.setter
    def stop_words(self, words):
        self._stop_words = frozenset(map(self.normalize_word, words))

#Using LexRank on a set of Sentences using Tf-idf metrics
def __call__(self, document, sentences_count):
        self._ensure_dependencies_installed()

        sentences_words = [self._to_words_set(s) for s in document.sentences]
        if not sentences_words:
            return tuple()

        tf_metrics = self._compute_tf(sentences_words)
        idf_metrics = self._compute_idf(sentences_words)

        matrix = self._create_matrix(sentences_words, self.threshold, tf_metrics, idf_metrics)
        scores = self.power_method(matrix, self.epsilon)
        ratings = dict(zip(document.sentences, scores))
        
        return self._get_best_sentences(document.sentences, sentences_count, ratings)

#Computing the tf values
def _compute_tf(self, sentences):
        tf_values = map(Counter, sentences)

        tf_metrics = []
        for sentence in tf_values:
            metrics = {}
            max_tf = self._find_tf_max(sentence)

            for term, tf in sentence.items():
                metrics[term] = tf / max_tf

            tf_metrics.append(metrics)

        return tf_metrics

#Creating the matrices
def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):
        """
        Creates matrix of shape |sentences|×|sentences|.
        """
        # create matrix |sentences|×|sentences| filled with zeroes
        sentences_count = len(sentences)
        matrix = numpy.zeros((sentences_count, sentences_count))
        degrees = numpy.zeros((sentences_count, ))

        for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):
            for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):
                matrix[row, col] = self.cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics)

                if matrix[row, col] > threshold:
                    matrix[row, col] = 1.0
                    degrees[row] += 1
                else:
                    matrix[row, col] = 0

        for row in range(sentences_count):
            for col in range(sentences_count):
                if degrees[row] == 0:
                    degrees[row] = 1

                matrix[row][col] = matrix[row][col] / degrees[row]

        return matrix

#Calculating the cosine values and Power values for eigen vectors
@staticmethod
def cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):

      unique_words1 = frozenset(sentence1)
      unique_words2 = frozenset(sentence2)
      common_words = unique_words1 & unique_words2

      numerator = 0.0
      for term in common_words:
          numerator += tf1[term]*tf2[term] * idf_metrics[term]**2

      denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in unique_words1)
      denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in unique_words2)

      if denominator1 > 0 and denominator2 > 0:
          return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))
      else:
          return 0.0

@staticmethod
def power_method(matrix, epsilon):
    transposed_matrix = matrix.T
    sentences_count = len(matrix)
    p_vector = numpy.array([1.0 / sentences_count] * sentences_count)
    lambda_val = 1.0

    while lambda_val > epsilon:
        next_p = numpy.dot(transposed_matrix, p_vector)
        lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))
        p_vector = next_p

    return p_vector

!pip install lexrank
!pip install path

#Importing the required packages
from lexrank import STOPWORDS, LexRank
from path import Path

#Mounting Google Colab
from google.colab import drive   
drive.mount('/content/drive')

#Setting path to the required set of documents available for summarization
documents = []
## The same package can be downloaded from the LexRank tutorials
documents_dir = Path('/content/drive/My Drive/politics')

for file_path in documents_dir.files('*.txt'):
    with file_path.open(mode='rt', encoding='utf-8') as fp:
        documents.append(fp.readlines())

lxr = LexRank(documents, stopwords=STOPWORDS['en'])

sentences = [
    'One of David Cameron\'s closest friends and Conservative allies, '
    'George Osborne rose rapidly after becoming MP for Tatton in 2001.',

    'Michael Howard promoted him from shadow chief secretary to the '
    'Treasury to shadow chancellor in May 2005, at the age of 34.',

    'Mr Osborne took a key role in the election campaign and has been at '
    'the forefront of the debate on how to deal with the recession and '
    'the UK\'s spending deficit.',

    'Even before Mr Cameron became leader the two were being likened to '
    'Labour\'s Blair/Brown duo. The two have emulated them by becoming '
    'prime minister and chancellor, but will want to avoid the spats.',

    'Before entering Parliament, he was a special adviser in the '
    'agriculture department when the Tories were in government and later '
    'served as political secretary to William Hague.',

    'The BBC understands that as chancellor, Mr Osborne, along with the '
    'Treasury will retain responsibility for overseeing banks and '
    'financial regulation.',

    'Mr Osborne said the coalition government was planning to change the '
    'tax system \"to make it fairer for people on low and middle '
    'incomes\", and undertake \"long-term structural reform\" of the '
    'banking sector, education and the welfare state.',
]

#Generating summary
summary = lxr.get_summary(sentences, summary_size=2, threshold=.1)
print(summary)

token1=[]
for words in sentences:
  temp_list1=words.split()
  for i in temp_list1:
    token1.append(i)
print(token1)

token2=[]
for words in summary:
  temp_list1=words.split()
  for i in temp_list1:
    token2.append(i)
print(token2)

import nltk
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer1=[]
for word in token1:
  ans1=(stemmer.stem(word))
  answer1.append(ans1)
print (answer1)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer1)
print(vectorizer.get_feature_names())

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer2=[]
for word in  token2:
  ans2=(stemmer.stem(word))
  answer2.append(ans2)
print (answer2)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer2)
print(vectorizer.get_feature_names())

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')  
l1 =[];l2 =[]

X_set = {w for w in token1 if not w in sw}  
Y_set = {w for w in token2 if not w in sw}

rvector = X_set.union(Y_set)  #Creating vector for calculating cosine similarity
for w in rvector: 
    if w in X_set: l1.append(1) # create a vector 
    else: l1.append(0) 
    if w in Y_set: l2.append(1) 
    else: l2.append(0) 
c = 0

for i in range(len(rvector)):    #Calculation of the cosine similarity
        c+= l1[i]*l2[i] 
cosine = c / float((sum(l1)*sum(l2))**0.5) 
print("similarity: ", cosine)

words1 = set(token1)  #Calculating Precision,Recall,F1-Score
words2 = set(token2)

intersections  = words1.intersection(words2)
len(intersections)
print("Precision:")
precision=len(intersections)/10
print(precision)
print("Recall:")
recall=len(intersections)/len(token1) 
print(recall)
print("F1 Score:")
F1_score=(2*(precision*recall))/precision+recall
print(F1_score)

"""**BERT Exractive Summarizer**"""

pip install bert-extractive-summarizer  #Installing BERT extractive summarizer

from summarizer import Summarizer  #Simple Example for BERT Extraction Summarizer

body = 'Text body that you want to summarize with BERT'
body2 = 'Something else you want to summarize with BERT'
model = Summarizer()
model(body)
model(body2)

from summarizer import Summarizer   #Number of sentences returned can be specified with the help of ratio specified
body = 'Text body that you want to summarize with BERT'
model = Summarizer()             #Here
result = model(body, ratio=0.2)  # Specified  ratio
result = model(body, num_sentences=3)  # Will return 3 sentences

from summarizer import Summarizer
model = Summarizer()
result = model.run_embeddings(body, ratio=0.2)  # Specified with ratio. 
result = model.run_embeddings(body, num_sentences=3)  # Will return (3, N) embedding numpy matrix.
result = model.run_embeddings(body, num_sentences=3, aggregate='mean')  # Will return Mean aggregate over embeddings.

from transformers import *

# Load model, model config and tokenizer via Transformers
custom_config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')
custom_config.output_hidden_states=True
custom_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')
custom_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=custom_config)

from summarizer import Summarizer   #Sample 

body = '''
The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price.
The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal.
Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008.
Real estate firm Tishman Speyer had owned the other 10%.
The buyer is RFR Holding, a New York real estate company.
Officials with Tishman and RFR did not immediately respond to a request for comments.
It's unclear when the deal will close.
The building sold fairly quickly after being publicly placed on the market only two months ago.
The sale was handled by CBRE Group.
The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building.
The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028.
Meantime, rents in the building itself are not rising nearly that fast.
While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor-to-ceiling windows and all the modern amenities.
Still the building is among the best known in the city, even to people who have never been to New York.
It is famous for its triangle-shaped, vaulted windows worked into the stylized crown, along with its distinctive eagle gargoyles near the top.
It has been featured prominently in many films, including Men in Black 3, Spider-Man, Armageddon, Two Weeks Notice and Independence Day.
The previous sale took place just before the 2008 financial meltdown led to a plunge in real estate prices.
Still there have been a number of high profile skyscrapers purchased for top dollar in recent years, including the Waldorf Astoria hotel, which Chinese firm Anbang Insurance purchased in 2016 for nearly $2 billion, and the Willis Tower in Chicago, which was formerly known as Sears Tower, once the world's tallest.
Blackstone Group (BX) bought it for $1.3 billion 2015.
The Chrysler Building was the headquarters of the American automaker until 1953, but it was named for and owned by Chrysler chief Walter Chrysler, not the company itself.
Walter Chrysler had set out to build the tallest building in the world, a competition at that time with another Manhattan skyscraper under construction at 40 Wall Street at the south end of Manhattan. He kept secret the plans for the spire that would grace the top of the building, building it inside the structure and out of view of the public until 40 Wall Street was complete.
Once the competitor could rise no higher, the spire of the Chrysler building was raised into view, giving it the title.
'''

model = Summarizer()
result = model(body, min_length=60)
full = ''.join(result)
print(full)
test= "The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price. The building sold fairly quickly after being publicly placed on the market only two months ago.The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building.'Still the building is among the best known in the city, even to people who have never been to New York."

tokens1=test.split()
print (tokens1)

tokens2=full.split()
print (tokens2)

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer1=[]
for word in  tokens1:
  ans1=(stemmer.stem(word))
  answer1.append(ans1)
print (answer1)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer1)
print(vectorizer.get_feature_names())

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer2=[]
for word in  tokens2:
  ans2=(stemmer.stem(word))
  answer2.append(ans2)
print (answer2)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer2)
print(vectorizer.get_feature_names())

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')  
l1 =[];l2 =[]

X_set = {w for w in tokens1 if not w in sw}  
Y_set = {w for w in tokens2 if not w in sw}

rvector = X_set.union(Y_set)  #Creating vector for calculating cosine similarity
for w in rvector: 
    if w in X_set: l1.append(1) # create a vector 
    else: l1.append(0) 
    if w in Y_set: l2.append(1) 
    else: l2.append(0) 
c = 0

for i in range(len(rvector)):    #Calculation of the cosine similarity
        c+= l1[i]*l2[i] 
cosine = c / float((sum(l1)*sum(l2))**0.5) 
print("similarity: ", cosine)

words1 = set(tokens1)  #Calculating Precision,Recall,F1-Score
words2 = set(tokens2)

intersections  = words1.intersection(words2)
len(intersections)
print("Precision:")
precision=len(intersections)/10
print(precision)
print("Recall:")
recall=len(intersections)/len(tokens1) 
print(recall)
print("F1 Score:")
F1_score=(2*(precision*recall))/precision+recall
print(F1_score)

"""**TextRank**"""

import numpy as np   #Importing Requried Libraries
import pandas as pd
import nltk
import re
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from scipy import spatial
import networkx as nx
import nltk
nltk.download('punkt')
nltk.download('stopwords')

text='''Santiago is a Shepherd who has a recurring dream which is supposedly prophetic. Inspired on learning this, he undertakes a journey to Egypt to discover the meaning of life and fulfill his destiny. During the course of his travels, he learns of his true purpose and meets many characters, including an “Alchemist”, that teach him valuable lessons about achieving his dreams. Santiago sets his sights on obtaining a certain kind of “treasure” for which he travels to Egypt. The key message is, “when you want something, all the universe conspires in helping you to achieve it.” Towards the final arc, Santiago gets robbed by bandits who end up revealing that the “treasure” he was looking for is buried in the place where his journey began. The end.'''

sentences=sent_tokenize(text)

sentences_clean=[re.sub(r'[^\w\s]','',sentence.lower()) for sentence in sentences]
stop_words = stopwords.words('english')
sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]

w2v=Word2Vec(sentence_tokens,size=1,min_count=1,iter=1000)
sentence_embeddings=[[w2v[word][0] for word in words] for words in sentence_tokens]
max_len=max([len(tokens) for tokens in sentence_tokens])
sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_embeddings]

similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])
for i,row_embedding in enumerate(sentence_embeddings):
    for j,column_embedding in enumerate(sentence_embeddings):
        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)

nx_graph = nx.from_numpy_array(similarity_matrix)
scores = nx.pagerank(nx_graph)

top_sentence={sentence:scores[index] for index,sentence in enumerate(sentences)}
top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:4])

for sent in sentences:   
    if sent in top.keys():
        print(sent)

tokens1=text.split()
print (tokens1)

token=[]
for sent in sentences:   
    if sent in top.keys():
        token.append(sent)

token2=[]
for words in token:
  temp_list1=words.split()
  for i in temp_list1:
    token2.append(i)
print (token2)

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer1=[]
for word in  tokens1:
  ans1=(stemmer.stem(word))
  answer1.append(ans1)
print (answer1)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer1)
print(vectorizer.get_feature_names())

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer2=[]
for word in  token2:
  ans2=(stemmer.stem(word))
  answer2.append(ans2)
print (answer2)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer2)
print(vectorizer.get_feature_names())

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')  
l1 =[];l2 =[]

X_set = {w for w in tokens1 if not w in sw}  
Y_set = {w for w in token2 if not w in sw}

rvector = X_set.union(Y_set)  #Creating vector for calculating cosine similarity
for w in rvector: 
    if w in X_set: l1.append(1) # create a vector 
    else: l1.append(0) 
    if w in Y_set: l2.append(1) 
    else: l2.append(0) 
c = 0

for i in range(len(rvector)):    #Calculation of the cosine similarity
        c+= l1[i]*l2[i] 
cosine = c / float((sum(l1)*sum(l2))**0.5) 
print("similarity: ", cosine)

words1 = set(tokens1)  #Calculating Precision,Recall,F1-Score
words2 = set(token2)

intersections  = words1.intersection(words2)
len(intersections)
print("Precision:")
precision=len(intersections)/10
print(precision)
print("Recall:")
recall=len(intersections)/len(tokens1) 
print(recall)
print("F1 Score:")
F1_score=(2*(precision*recall))/precision+recall
print(F1_score)

"""**Luhn Summarizer**"""

!pip install sumy

#Plain text parsers since we are parsing through text
from sumy.parsers.plaintext import PlaintextParser
#for tokenization
from sumy.nlp.tokenizers import Tokenizer

#Mounting Google Colab
from google.colab import drive   
drive.mount('/content/drive')

!pip install path

from path import Path

from sumy.summarizers.luhn import LuhnSummarizer #We're choosing Luhn, other algorithms are also built in

import numpy as np   #Importing Requried Libraries
import pandas as pd
import nltk
import re
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from scipy import spatial
import networkx as nx
import nltk
nltk.download('punkt')
nltk.download('stopwords')

file = Path('/content/drive/My Drive/politics/001.txt') #name of the plain-text file
parser = PlaintextParser.from_file(file, Tokenizer("english"))
summarizer_luhn = LuhnSummarizer()

summary = summarizer_luhn(parser.document, 5) #Summarize the document with 5 sentences

for sentence in summary:
    print (sentence)

test="Labour plans maternity pay rise Maternity pay for new mothers is to rise by £1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt.It would mean paid leave would be increased to nine months by 2007, Ms Hewitt told GMTV's Sunday programme. Other plans include letting maternity pay be given to fathers and extending rights to parents of older children. The Tories dismissed the maternity pay plan as 'desperate', while the Liberal Democrats said it was misdirected.Ms Hewitt said: 'We have already doubled the length of maternity pay, it was 13 weeks when we were elected, we have already taken it up to 26 weeks. 'We are going to extend the pay to nine months by 2007 and the aim is to get it right up to the full 12 months by the end of the next Parliament.' She said new mothers were already entitled to 12 months leave, but that many women could not take it as only six of those months were paid. 'We have made a firm commitment. We will definitely extend the maternity pay, from the six months where it now is to nine months, that's the extra £1,400.' She said ministers would consult on other proposals that could see fathers being allowed to take some of their partner's maternity pay or leave period, or extending the rights of flexible working to carers or parents of older children. The Shadow Secretary of State for the Family, Theresa May, said: 'These plans were announced by Gordon Brown in his pre-budget review in December and Tony Blair is now recycling it in his desperate bid to win back women voters.'She said the Conservatives would announce their proposals closer to the General Election. Liberal Democrat spokeswoman for women Sandra Gidley said: 'While mothers would welcome any extra maternity pay the Liberal Democrats feel this money is being misdirected.' She said her party would boost maternity pay in the first six months to allow more women to stay at home in that time.Ms Hewitt also stressed the plans would be paid for by taxpayers, not employers. But David Frost, director general of the British Chambers of Commerce, warned that many small firms could be 'crippled' by the move. 'While the majority of any salary costs may be covered by the government's statutory pay, recruitment costs, advertising costs, retraining costs and the strain on the company will not be,' he said. Further details of the government's plans will be outlined on Monday. New mothers are currently entitled to 90% of average earnings for the first six weeks after giving birth, followed by £102.80 a week until the baby is six months old."

tokens1=test.split()
print (tokens1)

s="Maternity pay for new mothers is to rise by £1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt. Ms Hewitt said: We have already doubled the length of maternity pay, it was 13 weeks when we were elected, we have already taken it up to 26 weeks.We are going to extend the pay to nine months by 2007 and the aim is to get it right up to the full 12 months by the end of the next Parliament. She said new mothers were already entitled to 12 months leave, but that many women could not take it as only six of those months were paid.She said ministers would consult on other proposals that could see fathers being allowed to take some of their partner's maternity pay or leave period, or extending the rights of flexible working to carers or parents of older children."

tokens2=s.split()
print(tokens2)

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer1=[]
for word in  tokens1:
  ans1=(stemmer.stem(word))
  answer1.append(ans1)
print (answer1)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer1)
print(vectorizer.get_feature_names())

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer2=[]
for word in  tokens2:
  ans2=(stemmer.stem(word))
  answer2.append(ans2)
print (answer2)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer2)
print(vectorizer.get_feature_names())

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')  
l1 =[];l2 =[]

X_set = {w for w in tokens1 if not w in sw}  
Y_set = {w for w in tokens2 if not w in sw}

rvector = X_set.union(Y_set)  #Creating vector for calculating cosine similarity
for w in rvector: 
    if w in X_set: l1.append(1) # create a vector 
    else: l1.append(0) 
    if w in Y_set: l2.append(1) 
    else: l2.append(0) 
c = 0

for i in range(len(rvector)):    #Calculation of the cosine similarity
        c+= l1[i]*l2[i] 
cosine = c / float((sum(l1)*sum(l2))**0.5) 
print("similarity: ", cosine)

words1 = set(tokens1)  #Calculating Precision,Recall,F1-Score
words2 = set(tokens2)

intersections  = words1.intersection(words2)
len(intersections)
print("Precision:")
precision=len(intersections)/10
print(precision)
print("Recall:")
recall=len(intersections)/len(tokens1) 
print(recall)
print("F1 Score:")
F1_score=(2*(precision*recall))/precision+recall
print(F1_score)

"""**LSA Summarizer**"""

!pip install sumy

#Plain text parsers since we are parsing through text
from sumy.parsers.plaintext import PlaintextParser
#for tokenization
from sumy.nlp.tokenizers import Tokenizer

#Mounting Google Colab
from google.colab import drive   
drive.mount('/content/drive')

!pip install path

from path import Path

from sumy.summarizers.lsa import LsaSummarizer

import numpy as np   #Importing Requried Libraries
import pandas as pd
import nltk
import re
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from scipy import spatial
import networkx as nx
import nltk
nltk.download('punkt')
nltk.download('stopwords')

file = Path('/content/drive/My Drive/politics/001.txt') #name of the plain-text file
parser = PlaintextParser.from_file(file, Tokenizer("english"))
summarizer_2 = LsaSummarizer()

summary =summarizer_2(parser.document,5)

for sentence in summary:
    print (sentence)

test="Labour plans maternity pay rise Maternity pay for new mothers is to rise by £1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt.It would mean paid leave would be increased to nine months by 2007, Ms Hewitt told GMTV's Sunday programme. Other plans include letting maternity pay be given to fathers and extending rights to parents of older children. The Tories dismissed the maternity pay plan as 'desperate', while the Liberal Democrats said it was misdirected.Ms Hewitt said: 'We have already doubled the length of maternity pay, it was 13 weeks when we were elected, we have already taken it up to 26 weeks. 'We are going to extend the pay to nine months by 2007 and the aim is to get it right up to the full 12 months by the end of the next Parliament.' She said new mothers were already entitled to 12 months leave, but that many women could not take it as only six of those months were paid. 'We have made a firm commitment. We will definitely extend the maternity pay, from the six months where it now is to nine months, that's the extra £1,400.' She said ministers would consult on other proposals that could see fathers being allowed to take some of their partner's maternity pay or leave period, or extending the rights of flexible working to carers or parents of older children. The Shadow Secretary of State for the Family, Theresa May, said: 'These plans were announced by Gordon Brown in his pre-budget review in December and Tony Blair is now recycling it in his desperate bid to win back women voters.'She said the Conservatives would announce their proposals closer to the General Election. Liberal Democrat spokeswoman for women Sandra Gidley said: 'While mothers would welcome any extra maternity pay the Liberal Democrats feel this money is being misdirected.' She said her party would boost maternity pay in the first six months to allow more women to stay at home in that time.Ms Hewitt also stressed the plans would be paid for by taxpayers, not employers. But David Frost, director general of the British Chambers of Commerce, warned that many small firms could be 'crippled' by the move. 'While the majority of any salary costs may be covered by the government's statutory pay, recruitment costs, advertising costs, retraining costs and the strain on the company will not be,' he said. Further details of the government's plans will be outlined on Monday. New mothers are currently entitled to 90% of average earnings for the first six weeks after giving birth, followed by £102.80 a week until the baby is six months old."

tokens1=test.split()
print (tokens1)

s="She said new mothers were already entitled to 12 months leave, but that many women could not take it as only six of those months were paid.Liberal Democrat spokeswoman for women Sandra Gidley said: 'While mothers would welcome any extra maternity pay the Liberal Democrats feel this money is being misdirected.'Ms Hewitt also stressed the plans would be paid for by taxpayers, not employers.Further details of the government's plans will be outlined on Monday.New mothers are currently entitled to 90% of average earnings for the first six weeks after giving birth, followed by £102.80 a week until the baby is six months old."

tokens2=s.split()
print(tokens2)

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer1=[]
for word in  tokens1:
  ans1=(stemmer.stem(word))
  answer1.append(ans1)
print (answer1)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer1)
print(vectorizer.get_feature_names())

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()        #Stemming using Porter Stemmer
answer2=[]
for word in  tokens2:
  ans2=(stemmer.stem(word))
  answer2.append(ans2)
print (answer2)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()        #Vectorizing the stemmed words
X = vectorizer.fit_transform(answer2)
print(vectorizer.get_feature_names())

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
sw = stopwords.words('english')  
l1 =[];l2 =[]

X_set = {w for w in tokens1 if not w in sw}  
Y_set = {w for w in tokens2 if not w in sw}

rvector = X_set.union(Y_set)  #Creating vector for calculating cosine similarity
for w in rvector: 
    if w in X_set: l1.append(1) # create a vector 
    else: l1.append(0) 
    if w in Y_set: l2.append(1) 
    else: l2.append(0) 
c = 0

for i in range(len(rvector)):    #Calculation of the cosine similarity
        c+= l1[i]*l2[i] 
cosine = c / float((sum(l1)*sum(l2))**0.5) 
print("similarity: ", cosine)

words1 = set(tokens1)  #Calculating Precision,Recall,F1-Score
words2 = set(tokens2)

intersections  = words1.intersection(words2)
len(intersections)
print("Precision:")
precision=len(intersections)/10
print(precision)
print("Recall:")
recall=len(intersections)/len(tokens1) 
print(recall)
print("F1 Score:")
F1_score=(2*(precision*recall))/precision+recall
print(F1_score)

